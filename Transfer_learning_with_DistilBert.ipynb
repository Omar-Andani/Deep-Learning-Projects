{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer learning with DistilBert",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer learning with DistilBert**\n",
        "\n",
        "The goal of this excercise is to build a text classifier using the pretrained DistilBert published by HuggingFace."
      ],
      "metadata": {
        "id": "cQ25iAgAOJ5K"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZj4c0xTeMwH"
      },
      "source": [
        "!pip install -q transformers tfds-nightly\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras as keras\n",
        "import pandas as pd\n",
        "\n",
        "try: # this is only working on the 2nd try in colab :)\n",
        "  from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "except Exception as err: # so we catch the error and import it again\n",
        "  from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "\n",
        "Clean the text and targets"
      ],
      "metadata": {
        "id": "jHvJJjnCRYF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(save_dir=\"./\"):\n",
        "  dataset = tfds.load('glue/cola', shuffle_files=True)\n",
        "  train = tfds.as_dataframe(dataset[\"train\"])\n",
        "  val = tfds.as_dataframe(dataset[\"validation\"])\n",
        "  test = tfds.as_dataframe(dataset[\"test\"])\n",
        "  return train, val, test\n",
        "\n",
        "def prepare_raw_data(df):\n",
        "  raw_data = df.loc[:, [\"idx\", \"sentence\", \"label\"]]\n",
        "  raw_data[\"label\"] = raw_data[\"label\"].astype('category')\n",
        "  return raw_data\n",
        "\n",
        "train, val, test = load_data()\n",
        "train = prepare_raw_data(train)\n",
        "val = prepare_raw_data(val)\n",
        "test = prepare_raw_data(test)"
      ],
      "metadata": {
        "id": "q3gYLKfEd0Hb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df):\n",
        "  clean_data = df.drop_duplicates(subset=[\"sentence\", \"label\"])\n",
        "  clean_data = clean_data.drop_duplicates(subset=[\"sentence\"], keep=False)\n",
        "  clean_data['token_count'] = [len(x.split()) for x in clean_data.sentence]\n",
        "  clean_data = clean_data[clean_data['token_count'] >= 5]\n",
        "  return clean_data\n",
        "\n",
        "train = clean_data(train)\n",
        "val = clean_data(val)\n",
        "test = clean_data(test)\n",
        "\n",
        "print(train.head())\n",
        "print(test.head())"
      ],
      "metadata": {
        "id": "xHNJC2vZmTWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675b4c18-e673-450b-a02b-b9dc6d98665a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    idx                                           sentence label  token_count\n",
            "0  1680  b'It is this hat that it is certain that he wa...     1           12\n",
            "1  1456  b'Her efficient looking up of the answer pleas...     1           10\n",
            "2  4223          b'Both the workers will wear carnations.'     1            6\n",
            "3  4093  b'John enjoyed drawing trees for his syntax ho...     1            8\n",
            "4  7111  b'We consider Leslie rather foolish, and Lou a...     1           10\n",
            "    idx                                           sentence label  token_count\n",
            "0   163              b'Brian was wiping behind the stove.'    -1            6\n",
            "1   131         b'You could give a headache to a Tylenol.'    -1            8\n",
            "2  1021                            b'I want to meet at 6.'    -1            6\n",
            "4  1039    b\"Many people said they were sick who weren't.\"    -1            8\n",
            "5   778  b'A dog with brown spots chased a cat with no ...    -1           11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the text for DistilBert"
      ],
      "metadata": {
        "id": "MAYstlfPQSvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_and_y(df):\n",
        "  text = [x.decode('utf-8') for x in  df.sentence.values]\n",
        "  # for multiclass problems, you can use sklearn.preprocessing.OneHotEncoder, but we only have two classes, so we'll use a single sigmoid output\n",
        "  y = np.array([x for x in df.label.values])\n",
        "  return text, y\n",
        "\n",
        "def encode_text(text):\n",
        "    model_inputs_and_masks = dbert_tokenizer(\n",
        "        text, \n",
        "        return_tensors=\"tf\",\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=100\n",
        "    )\n",
        "    input_ids = model_inputs_and_masks['input_ids']\n",
        "    attention_mask = model_inputs_and_masks['attention_mask']\n",
        "\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "# the following prepares the input for running in DistilBert\n",
        "train_text, train_y = extract_text_and_y(clean_data(train))\n",
        "val_text, val_y = extract_text_and_y(clean_data(val))\n",
        "test_text, test_y = extract_text_and_y(clean_data(test))\n",
        "\n",
        "train_input, train_mask = encode_text(train_text)\n",
        "val_input, val_mask = encode_text(val_text)\n",
        "test_input, test_mask = encode_text(test_text)\n",
        "\n",
        "train_model_inputs_and_masks = {\n",
        "    'inputs' : train_input,\n",
        "    'masks' : train_mask\n",
        "}\n",
        "\n",
        "val_model_inputs_and_masks = {\n",
        "    'inputs' : val_input,\n",
        "    'masks' : val_mask\n",
        "}\n",
        "\n",
        "test_model_inputs_and_masks = {\n",
        "    'inputs' : test_input,\n",
        "    'masks' : test_mask\n",
        "}"
      ],
      "metadata": {
        "id": "_RBthPA0fcTA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2USajN2MWjn"
      },
      "source": [
        "# Modelling\n",
        "\n",
        "## Build and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(base_model, trainable=False, params={}):\n",
        "    # build the model, with the option to freeze the parameters in distilBERT\n",
        "    # the cls token corresponds to the first element in the sequence in DistilBert\n",
        "\n",
        "    max_seq_len = params[\"max_seq_len\"]\n",
        "    inputs = Input(shape = (max_seq_len,), dtype='int64', name='inputs')\n",
        "    masks  = Input(shape = (max_seq_len,), dtype='int64', name='masks')\n",
        "\n",
        "    base_model.trainable = trainable\n",
        "\n",
        "    dbert_output = base_model(inputs, attention_mask=masks)\n",
        "    dbert_last_hidden_state = dbert_output.last_hidden_state\n",
        "\n",
        "    # add additional layers\n",
        "    # 'params' as dictionary for hyperparameter in experiments\n",
        "\n",
        "    dbert_cls_output = dbert_last_hidden_state[:,0,:]\n",
        "\n",
        "    my_output = Dense(params[\"layer_width1\"], activation='relu')(dbert_cls_output)\n",
        "    my_output = Dropout(params[\"dropout1\"])(my_output)\n",
        "    my_output = Dense(params[\"layer_width2\"], activation='relu')(my_output)\n",
        "    my_output = Dropout(params[\"dropout2\"])(my_output)\n",
        "\n",
        "    probs = Dense(1, activation='sigmoid')(my_output)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs, masks], outputs=probs)\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "params={\"max_seq_len\" : train_input.shape[1],\n",
        "        \"layer_width1\" : 128,\n",
        "        \"dropout1\" : 0.3,\n",
        "        \"layer_width2\" : 64,\n",
        "        \"dropout2\" : 0.3}\n",
        "\n",
        "model = build_model(dbert_model, params=params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO5OWYR-xmff",
        "outputId": "1bedf5a5-36c1-47a6-9671-28b910655cd5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inputs (InputLayer)            [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " masks (InputLayer)             [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model_4 (TFDist  TFBaseModelOutput(l  66362880   ['inputs[0][0]',                 \n",
            " ilBertModel)                   ast_hidden_state=(N               'masks[0][0]']                  \n",
            "                                one, 100, 768),                                                   \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_4 (Sl  (None, 768)         0           ['tf_distil_bert_model_4[0][0]'] \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 128)          98432       ['tf.__operators__.getitem_4[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_103 (Dropout)          (None, 128)          0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 64)           8256        ['dropout_103[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_104 (Dropout)          (None, 64)           0           ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 1)            65          ['dropout_104[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66,469,633\n",
            "Trainable params: 106,753\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_model(model):\n",
        "    # include relevant auc metrics when training\n",
        "    \n",
        "    model.compile(\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        metrics=[\n",
        "            'accuracy', \n",
        "            keras.metrics.AUC(curve=\"ROC\", multi_label=True), \n",
        "            keras.metrics.AUC(curve=\"PR\", multi_label=True), \n",
        "            keras.metrics.Precision(),\n",
        "            keras.metrics.Recall()\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = compile_model(model)"
      ],
      "metadata": {
        "id": "Z3EyvQbSzu5m"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, model_inputs_and_masks_train, model_inputs_and_masks_val, y_train, y_val, batch_size, num_epochs):\n",
        "    es = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", \n",
        "        mode='min', \n",
        "        verbose=1,\n",
        "        patience=1\n",
        "    )\n",
        "    history = model.fit(\n",
        "            model_inputs_and_masks_train, \n",
        "            y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=num_epochs,\n",
        "            verbose=1,\n",
        "            validation_data=(model_inputs_and_masks_val, y_val),\n",
        "            callbacks=[es]\n",
        "        )\n",
        "    return model, history\n",
        "\n",
        "model, history = train_model(model, train_model_inputs_and_masks, val_model_inputs_and_masks, train_y, val_y, batch_size=128, num_epochs=15)"
      ],
      "metadata": {
        "id": "Nz8kT3f8zykl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "915fefdd-1290-443d-fe9e-6b0a3a291d4c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "57/57 [==============================] - 142s 910ms/step - loss: 0.6285 - accuracy: 0.6869 - auc_12: 0.5094 - auc_13: 0.7141 - precision_6: 0.7059 - recall_6: 0.9535 - val_loss: 0.6013 - val_accuracy: 0.7030 - val_auc_12: 0.6010 - val_auc_13: 0.7567 - val_precision_6: 0.7030 - val_recall_6: 1.0000\n",
            "Epoch 2/15\n",
            "57/57 [==============================] - 49s 861ms/step - loss: 0.6097 - accuracy: 0.7042 - auc_12: 0.5408 - auc_13: 0.7315 - precision_6: 0.7062 - recall_6: 0.9943 - val_loss: 0.5963 - val_accuracy: 0.7030 - val_auc_12: 0.6217 - val_auc_13: 0.7813 - val_precision_6: 0.7030 - val_recall_6: 1.0000\n",
            "Epoch 3/15\n",
            "57/57 [==============================] - 50s 874ms/step - loss: 0.6028 - accuracy: 0.7054 - auc_12: 0.5641 - auc_13: 0.7496 - precision_6: 0.7063 - recall_6: 0.9971 - val_loss: 0.5915 - val_accuracy: 0.7030 - val_auc_12: 0.6397 - val_auc_13: 0.7928 - val_precision_6: 0.7030 - val_recall_6: 1.0000\n",
            "Epoch 4/15\n",
            "57/57 [==============================] - 49s 864ms/step - loss: 0.5986 - accuracy: 0.7049 - auc_12: 0.5822 - auc_13: 0.7597 - precision_6: 0.7060 - recall_6: 0.9967 - val_loss: 0.5878 - val_accuracy: 0.7030 - val_auc_12: 0.6500 - val_auc_13: 0.8021 - val_precision_6: 0.7030 - val_recall_6: 1.0000\n",
            "Epoch 5/15\n",
            "57/57 [==============================] - 49s 862ms/step - loss: 0.5933 - accuracy: 0.7055 - auc_12: 0.6002 - auc_13: 0.7734 - precision_6: 0.7067 - recall_6: 0.9959 - val_loss: 0.5835 - val_accuracy: 0.7053 - val_auc_12: 0.6502 - val_auc_13: 0.8035 - val_precision_6: 0.7046 - val_recall_6: 1.0000\n",
            "Epoch 6/15\n",
            "57/57 [==============================] - 49s 860ms/step - loss: 0.5924 - accuracy: 0.7072 - auc_12: 0.6045 - auc_13: 0.7732 - precision_6: 0.7084 - recall_6: 0.9941 - val_loss: 0.5800 - val_accuracy: 0.7053 - val_auc_12: 0.6589 - val_auc_13: 0.8098 - val_precision_6: 0.7046 - val_recall_6: 1.0000\n",
            "Epoch 7/15\n",
            "57/57 [==============================] - 49s 862ms/step - loss: 0.5866 - accuracy: 0.7051 - auc_12: 0.6247 - auc_13: 0.7871 - precision_6: 0.7072 - recall_6: 0.9932 - val_loss: 0.5780 - val_accuracy: 0.7053 - val_auc_12: 0.6634 - val_auc_13: 0.8116 - val_precision_6: 0.7046 - val_recall_6: 1.0000\n",
            "Epoch 8/15\n",
            "57/57 [==============================] - 49s 862ms/step - loss: 0.5865 - accuracy: 0.7069 - auc_12: 0.6217 - auc_13: 0.7863 - precision_6: 0.7088 - recall_6: 0.9920 - val_loss: 0.5756 - val_accuracy: 0.7076 - val_auc_12: 0.6701 - val_auc_13: 0.8143 - val_precision_6: 0.7062 - val_recall_6: 1.0000\n",
            "Epoch 9/15\n",
            "57/57 [==============================] - 49s 860ms/step - loss: 0.5815 - accuracy: 0.7084 - auc_12: 0.6381 - auc_13: 0.7942 - precision_6: 0.7097 - recall_6: 0.9928 - val_loss: 0.5712 - val_accuracy: 0.7110 - val_auc_12: 0.6737 - val_auc_13: 0.8148 - val_precision_6: 0.7096 - val_recall_6: 0.9967\n",
            "Epoch 10/15\n",
            "57/57 [==============================] - 49s 861ms/step - loss: 0.5784 - accuracy: 0.7101 - auc_12: 0.6479 - auc_13: 0.8017 - precision_6: 0.7135 - recall_6: 0.9842 - val_loss: 0.5702 - val_accuracy: 0.7122 - val_auc_12: 0.6733 - val_auc_13: 0.8143 - val_precision_6: 0.7114 - val_recall_6: 0.9935\n",
            "Epoch 11/15\n",
            "57/57 [==============================] - 49s 860ms/step - loss: 0.5748 - accuracy: 0.7095 - auc_12: 0.6587 - auc_13: 0.8043 - precision_6: 0.7138 - recall_6: 0.9818 - val_loss: 0.5694 - val_accuracy: 0.7122 - val_auc_12: 0.6789 - val_auc_13: 0.8174 - val_precision_6: 0.7105 - val_recall_6: 0.9967\n",
            "Epoch 12/15\n",
            "57/57 [==============================] - 49s 860ms/step - loss: 0.5754 - accuracy: 0.7163 - auc_12: 0.6535 - auc_13: 0.8015 - precision_6: 0.7192 - recall_6: 0.9807 - val_loss: 0.5713 - val_accuracy: 0.7110 - val_auc_12: 0.6808 - val_auc_13: 0.8178 - val_precision_6: 0.7101 - val_recall_6: 0.9951\n",
            "Epoch 12: early stopping\n"
          ]
        }
      ]
    }
  ]
}