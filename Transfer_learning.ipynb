{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer learning",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer learning with DistilBert**\n",
        "\n",
        "The goal of this excercise is to build a text classifier using the pretrained DistilBert published by HuggingFace."
      ],
      "metadata": {
        "id": "cQ25iAgAOJ5K"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZj4c0xTeMwH"
      },
      "source": [
        "!pip install -q transformers tfds-nightly\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras as keras\n",
        "import pandas as pd\n",
        "\n",
        "try: # this is only working on the 2nd try in colab :)\n",
        "  from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "except Exception as err: # so we catch the error and import it again\n",
        "  from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "\n",
        "Clean the text and targets"
      ],
      "metadata": {
        "id": "jHvJJjnCRYF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(save_dir=\"./\"):\n",
        "  dataset = tfds.load('glue/cola', shuffle_files=True)\n",
        "  train = tfds.as_dataframe(dataset[\"train\"])\n",
        "  val = tfds.as_dataframe(dataset[\"validation\"])\n",
        "  test = tfds.as_dataframe(dataset[\"test\"])\n",
        "  return train, val, test\n",
        "\n",
        "def prepare_raw_data(df):\n",
        "  raw_data = df.loc[:, [\"idx\", \"sentence\", \"label\"]]\n",
        "  raw_data[\"label\"] = raw_data[\"label\"].astype('category')\n",
        "  return raw_data\n",
        "\n",
        "train, val, test = load_data()\n",
        "train = prepare_raw_data(train)\n",
        "val = prepare_raw_data(val)\n",
        "test = prepare_raw_data(test)"
      ],
      "metadata": {
        "id": "q3gYLKfEd0Hb"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df):\n",
        "  clean_data = df.drop_duplicates(subset=[\"sentence\", \"label\"])\n",
        "  clean_data = clean_data.drop_duplicates(subset=[\"sentence\"], keep=False)\n",
        "  clean_data['token_count'] = [len(x.split()) for x in clean_data.sentence]\n",
        "  clean_data = clean_data[clean_data['token_count'] >= 10]\n",
        "  return clean_data\n",
        "\n",
        "train = clean_data(train)\n",
        "val = clean_data(val)\n",
        "test = clean_data(test)\n",
        "\n",
        "print(train.head())\n",
        "print(test.head())"
      ],
      "metadata": {
        "id": "xHNJC2vZmTWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "256ff5ed-8e03-4be2-be88-1e1a617555aa"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    idx                                           sentence label  token_count\n",
            "0  1680  b'It is this hat that it is certain that he wa...     1           12\n",
            "1  1456  b'Her efficient looking up of the answer pleas...     1           10\n",
            "4  7111  b'We consider Leslie rather foolish, and Lou a...     1           10\n",
            "7  5242  b\"I didn't help him because I have any sympath...     1           12\n",
            "8  7137  b'Bill will put a picture of her on your desk ...     1           12\n",
            "    idx                                           sentence label  token_count\n",
            "5   778  b'A dog with brown spots chased a cat with no ...    -1           11\n",
            "7   668  b'Tom swam the English Channel because he beli...    -1           11\n",
            "11  554  b\"Joan said she talked to some students but I ...    -1           12\n",
            "12  709  b'Kim gave a book to Sandy and a record to Dana.'    -1           11\n",
            "13  592  b\"Bob found a plumber to fix the sink but it's...    -1           14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the text for DistilBert"
      ],
      "metadata": {
        "id": "MAYstlfPQSvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_and_y(df):\n",
        "  text = [x.decode('utf-8') for x in  df.sentence.values]\n",
        "  # for multiclass problems, you can use sklearn.preprocessing.OneHotEncoder, but we only have two classes, so we'll use a single sigmoid output\n",
        "  y = np.array([x for x in df.label.values])\n",
        "  return text, y\n",
        "\n",
        "def encode_text(text):\n",
        "    model_inputs_and_masks = dbert_tokenizer(\n",
        "        text, \n",
        "        return_tensors=\"tf\",\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=100\n",
        "    )\n",
        "    input_ids = model_inputs_and_masks['input_ids']\n",
        "    attention_mask = model_inputs_and_masks['attention_mask']\n",
        "\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "# the following prepares the input for running in DistilBert\n",
        "train_text, train_y = extract_text_and_y(clean_data(train))\n",
        "val_text, val_y = extract_text_and_y(clean_data(val))\n",
        "test_text, test_y = extract_text_and_y(clean_data(test))\n",
        "\n",
        "train_input, train_mask = encode_text(train_text)\n",
        "val_input, val_mask = encode_text(val_text)\n",
        "test_input, test_mask = encode_text(test_text)\n",
        "\n",
        "train_model_inputs_and_masks = {\n",
        "    'inputs' : train_input,\n",
        "    'masks' : train_mask\n",
        "}\n",
        "\n",
        "val_model_inputs_and_masks = {\n",
        "    'inputs' : val_input,\n",
        "    'masks' : val_mask\n",
        "}\n",
        "\n",
        "test_model_inputs_and_masks = {\n",
        "    'inputs' : test_input,\n",
        "    'masks' : test_mask\n",
        "}"
      ],
      "metadata": {
        "id": "_RBthPA0fcTA"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2USajN2MWjn"
      },
      "source": [
        "# Modelling\n",
        "\n",
        "## Build and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(base_model, trainable=False, params={}):\n",
        "    # build the model, with the option to freeze the parameters in distilBERT\n",
        "    # the cls token corresponds to the first element in the sequence in DistilBert\n",
        "\n",
        "    max_seq_len = params[\"max_seq_len\"]\n",
        "    inputs = Input(shape = (max_seq_len,), dtype='int64', name='inputs')\n",
        "    masks  = Input(shape = (max_seq_len,), dtype='int64', name='masks')\n",
        "\n",
        "    base_model.trainable = trainable\n",
        "\n",
        "    dbert_output = base_model(inputs, attention_mask=masks)\n",
        "    dbert_last_hidden_state = dbert_output.last_hidden_state\n",
        "\n",
        "    # add additional layers\n",
        "    # 'params' as dictionary for hyperparameter in experiments\n",
        "\n",
        "    dbert_cls_output = dbert_last_hidden_state[:,0,:]\n",
        "\n",
        "    my_output = Dense(params[\"layer_width1\"], activation='relu')(dbert_cls_output)\n",
        "    my_output = Dropout(params[\"dropout1\"])(my_output)\n",
        "    my_output = Dense(params[\"layer_width2\"], activation='relu')(my_output)\n",
        "    my_output = Dropout(params[\"dropout2\"])(my_output)\n",
        "\n",
        "    probs = Dense(1, activation='sigmoid')(my_output)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs, masks], outputs=probs)\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "params={\"max_seq_len\" : train_input.shape[1],\n",
        "        \"layer_width1\" : 128,\n",
        "        \"dropout1\" : 0.3,\n",
        "        \"layer_width2\" : 64,\n",
        "        \"dropout2\" : 0.3}\n",
        "\n",
        "model = build_model(dbert_model, params=params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO5OWYR-xmff",
        "outputId": "5e788089-922e-465c-d612-9476a1fad232"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inputs (InputLayer)            [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " masks (InputLayer)             [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model_12 (TFDis  TFBaseModelOutput(l  66362880   ['inputs[0][0]',                 \n",
            " tilBertModel)                  ast_hidden_state=(N               'masks[0][0]']                  \n",
            "                                one, 100, 768),                                                   \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_distil_bert_model_12[0][0]']\n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 128)          98432       ['tf.__operators__.getitem_5[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_257 (Dropout)          (None, 128)          0           ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 64)           8256        ['dropout_257[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_258 (Dropout)          (None, 64)           0           ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 1)            65          ['dropout_258[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66,469,633\n",
            "Trainable params: 106,753\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_model(model):\n",
        "    # include relevant auc metrics when training\n",
        "    \n",
        "    model.compile(\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
        "        metrics=[\n",
        "            'accuracy', \n",
        "            keras.metrics.AUC(curve=\"ROC\", multi_label=True), \n",
        "            keras.metrics.AUC(curve=\"PR\", multi_label=True), \n",
        "            keras.metrics.Precision(),\n",
        "            keras.metrics.Recall()\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = compile_model(model)"
      ],
      "metadata": {
        "id": "Z3EyvQbSzu5m"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, model_inputs_and_masks_train, model_inputs_and_masks_val, y_train, y_val, batch_size, num_epochs):\n",
        "    es = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", \n",
        "        mode='min', \n",
        "        verbose=1,\n",
        "        patience=1\n",
        "    )\n",
        "    history = model.fit(\n",
        "            model_inputs_and_masks_train, \n",
        "            y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=num_epochs,\n",
        "            verbose=1,\n",
        "            validation_data=(model_inputs_and_masks_val, y_val),\n",
        "            callbacks=[es]\n",
        "        )\n",
        "    return model, history\n",
        "\n",
        "model, history = train_model(model, train_model_inputs_and_masks, val_model_inputs_and_masks, train_y, val_y, batch_size=128, num_epochs=10)"
      ],
      "metadata": {
        "id": "Nz8kT3f8zykl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb86a53-2f51-4dac-e4b9-09589c0ed2ad"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "16/16 [==============================] - 27s 1s/step - loss: 0.6617 - accuracy: 0.6266 - auc_12: 0.4769 - auc_13: 0.6806 - precision_6: 0.6944 - recall_6: 0.8316 - val_loss: 0.6324 - val_accuracy: 0.6824 - val_auc_12: 0.4761 - val_auc_13: 0.6490 - val_precision_6: 0.6824 - val_recall_6: 1.0000\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 13s 848ms/step - loss: 0.6482 - accuracy: 0.6358 - auc_12: 0.5052 - auc_13: 0.7041 - precision_6: 0.6940 - recall_6: 0.8565 - val_loss: 0.6303 - val_accuracy: 0.6824 - val_auc_12: 0.4762 - val_auc_13: 0.6497 - val_precision_6: 0.6824 - val_recall_6: 1.0000\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 14s 853ms/step - loss: 0.6354 - accuracy: 0.6624 - auc_12: 0.5193 - auc_13: 0.7163 - precision_6: 0.7017 - recall_6: 0.8990 - val_loss: 0.6296 - val_accuracy: 0.6824 - val_auc_12: 0.4808 - val_auc_13: 0.6530 - val_precision_6: 0.6824 - val_recall_6: 1.0000\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 14s 857ms/step - loss: 0.6353 - accuracy: 0.6747 - auc_12: 0.5187 - auc_13: 0.7063 - precision_6: 0.7053 - recall_6: 0.9180 - val_loss: 0.6294 - val_accuracy: 0.6824 - val_auc_12: 0.4847 - val_auc_13: 0.6553 - val_precision_6: 0.6824 - val_recall_6: 1.0000\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 14s 861ms/step - loss: 0.6414 - accuracy: 0.6716 - auc_12: 0.4908 - auc_13: 0.7041 - precision_6: 0.6980 - recall_6: 0.9341 - val_loss: 0.6296 - val_accuracy: 0.6824 - val_auc_12: 0.4856 - val_auc_13: 0.6575 - val_precision_6: 0.6824 - val_recall_6: 1.0000\n",
            "Epoch 5: early stopping\n"
          ]
        }
      ]
    }
  ]
}